# ğŸ›¡ï¸ Backdoor Defense Papers Collection

> ğŸ“š **A comprehensive collection of research papers on backdoor defenses in machine learning.**  
> ğŸ“ **Maintained by**: Dr. [Minwei Wen] | [Chongqing University of Posts and Telecommunications] | [2058477814@qq.com]

## ğŸ“– About This Repository

This repository serves as a curated collection of academic papers focusing on **defense mechanisms** in machine learning. Our goal is to provide researchers, practitioners, and students with a comprehensive overview of the current state-of-the-art in this critical security domain.


### ğŸ¯ **Repository Purpose:**
- ğŸ† **Quality Focus**: Emphasis on high-impact venues. [CCF-Rankings](https://www.ccf.org.cn/en/About_CCF/Media_Center/) now marked with different colors(![arXiv](https://img.shields.io/badge/CCF_A-dc3545) ![Static Badge](https://img.shields.io/badge/CCF_B-ffc107) ![Static Badge](https://img.shields.io/badge/CCF_C-28a745) ![Static Badge](https://img.shields.io/badge/CCF_None-6c757d))
- ğŸ”„ **Regular Updates**: Continuously updated with latest research developments
- ğŸŒ **Easy Access**: Direct links to papers, code repositories, and supplementary materials

### ğŸ“Š **Each paper includes the following evaluation metrics (out of 5 stars):**
- **ğŸ’¡ Motivation**: How well-motivated and significant is the research problem? â­â­â­â­â­ (5/5)
- **ğŸ”§ Method**: How novel and technically sound is the proposed approach? â­â­â­â­â­ (5/5)

<h2 id="awesome-papers"> ğŸ‘‘ Awesome Papers List </h2>

<h3 id="attacks"> 2023 </h3>

* **[BaDExpert: Extracting Backdoor Functionality for Accurate Backdoor Input Detection](https://openreview.net/forum?id=s56xikpD92)** ![Static Badge](https://img.shields.io/badge/ICLR'23-6c757d) [![GitHub stars](https://img.shields.io/github/stars/vtu81/backdoor-toolbox?style=social)]([https://github.com/yunqing-me/AttackVLM](https://github.com/vtu81/backdoor-toolbox)) 
  * Yunqing Zhao, Tianyu Pang, Chao Du, Xiao Yang, Chongxuan Li, Ngai-Man Cheung, Min Lin
  * **ğŸ“ Summary**: A novel approach that extracts backdoor functionality from infected models to enable accurate detection of backdoor inputs without requiring clean reference data.
  * **ğŸ’¡ Motivation**: â­â­â­â­â­ (5/5) - Addresses the critical challenge of backdoor detection in practical scenarios where clean data is unavailable
  * **ğŸ”§ Method**: â­â­â­â­â­ (5/5) - Innovative backdoor functionality extraction technique with strong theoretical foundation and empirical validation

<h3 id="attacks"> 2024 </h3>

* **[Towards Reliable and Efficient Backdoor Trigger Inversion via Decoupling Benign Features](https://openreview.net/forum?id=Tw9wemV6cb)** ![Static Badge](https://img.shields.io/badge/ICLR'24-6c757d) [![GitHub stars](https://img.shields.io/github/stars/xuxiong0214/BTIDBF?style=social)](https://github.com/xuxiong0214/BTIDBF)
  * Xiong Xu1, Kunzhe Huang, Yiming Li, Zhan Qin1, Kui Ren
  * **ğŸ“ Summary**: Address the limitation of existing backdoor trigger inversion methods, where the generated triggers differ significantly from the actual triggers used by attackers.
  * **ğŸ’¡ Motivation**: â­â­â­â­â­ (5/5) - Overcome the reliance of current methods on extracting backdoor features for trigger inversion.
  * **ğŸ”§ Method**: â­â­â­ (3/5) - Mitigate the poor performance of current methods against backdoor attacks with only a minor footprint in the feature space.

* **[BAN: Detecting Backdoors Activated by Adversarial Neuron Noise](https://openreview.net/forum?id=Tw9wemV6cb)** ![Static Badge](https://img.shields.io/badge/NeurIPS'24-6c757d)![arXiv](https://img.shields.io/badge/CCF_A-dc3545) [![GitHub stars](https://img.shields.io/github/stars/xiaoyunxxy/ban?style=social)](https://github.com/xiaoyunxxy/ban)
  * Xiaoyun Xu, Zhuoran Liu, Stefanos Koffas, Shujian Yu, Stjepan Picek
  * **ğŸ“ Summary**: Address the excessive reliance of existing methods on the separability between benign and backdoor features, which also incurs high computational cost.
  * **ğŸ’¡ Motivation**: â­â­â­â­â­ (5/5) - Observe that backdoored models are more sensitive than clean models to adversarial noise, and that neuron-level noise can be adversarially manipulated to reveal backdoor activations.
  * **ğŸ”§ Method**: â­â­â­â­â­ (3/5) - Leverage neuron noise to substantially amplify backdoor effects, thereby improving the effectiveness of feature decoupling.
    
<h3 id="attacks"> 2025 </h3>

* **[Adversarial-Inspired Backdoor Defense via Bridging Backdoor and Adversarial Attacks](https://ojs.aaai.org/index.php/AAAI/article/view/33030)** ![Static Badge](https://img.shields.io/badge/AAAI'25-6c757d)![arXiv](https://img.shields.io/badge/CCF_A-dc3545)
  * Jia-Li Yin, Weijian Wang, Lyhwa, Wei Lin, Ximeng Liu
  * **ğŸ“ Summary**: Leveraging the intrinsic connection between adversarial attacks and backdoor attacks to effectively isolate poisoned samples and weaken the association between backdoor triggers and target labels.
  * **ğŸ’¡ Motivation**: â­â­â­â­â­ (5/5) - 1) the sample is harder to be turned into an adversarial examples when the trigger is presented; 2) the adversarial examples generated from backdoor samples are highly likely to be predicted as its true labels.
  * **ğŸ”§ Method**: â­â­â­â­ (4/5) - Designed a clever top-q strategy to select poisoned samples.

* **[Activation Gradient based Poisoned Sample Detection Against Backdoor Attacks](https://openreview.net/forum?id=VNMJfBBUd5)** ![Static Badge](https://img.shields.io/badge/ICLR'25-6c757d)
  * Danni Yuan, Shaokui Wei, Mingda Zhang, Li Liu, Baoyuan Wu
  * **ğŸ“ Summary**: By analyzing the distribution differences of activation gradients in the model, it effectively identifies target classes and distinguishes between clean and poisoned samples.
  * **ğŸ’¡ Motivation**: â­â­â­â­â­ (4/5) - Backdoored models tend to map significantly different poisoned samples and clean samples to similar activation regions.
  * **ğŸ”§ Method**: â­â­â­â­ (4/5) - Designed a novel class-level and sample-level selection method.
